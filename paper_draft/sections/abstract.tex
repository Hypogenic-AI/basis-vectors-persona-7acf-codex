Persona steering in large language models is often driven by manually crafted vectors, but it remains unclear whether persona representations share a compact, reusable basis in residual space. We test this question on a real pretrained model (\qwen) using three datasets: \personahub for vector-bank construction, \personachat for behavioral evaluation, and \mypersonality for external trait alignment. Our pipeline extracts persona-conditioned residual vectors, fits PCA, and evaluates geometry, stability, and causal interventions.

We find strong low-rank structure: the top 10 principal components (PCs) explain 57.14\% of variance and the top 20 explain 70.06\%. Subspace stability is high across bootstrap splits (mean overlap 0.905 $\pm$ 0.016), and variance capture substantially exceeds random 10D bases. However, causal steering gains are limited. In directional Wilcoxon tests, \pcminus versus base is significant ($p=0.0228$, $d=-0.309$), while \pcplus does not significantly outperform either base or \randplus ($p=0.1587$ for both comparisons). External validity is weak: no PC--Big-5 correlations survive Benjamini-Hochberg correction, and the maximum absolute raw correlation is $|r|=0.0918$.

These results support a clear geometric claim: persona vectors occupy a stable low-dimensional subspace. At the same time, they caution against treating top PCs as universally reliable control knobs. We conclude that PCA is promising for persona analysis and compression, but robust persona control needs stronger objectives and richer evaluation metrics.
